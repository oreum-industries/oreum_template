# README_DATA.md

## General Notes

+ This is an R&D project that moves rapidly and doesn't assume system
  integration or DB access, so we typically expect to work on static datasets
  stored locally for development
+ Datafiles should **_never_** be committed to the repo
+ If we must use any local datafiles, they are explicitly excluded from the repo
  using the `.gitignore` file
+ Use [Git LFS](https://git-lfs.github.com) to track any large files, and
  control this using  `.gitattributes` files
  e.g. `$> git lfs track data/raw/verbatim/abc.csv`
+ Data files are typically in `.parquet` format for optimised storage
+ For ease of use, we suggest using the cross-platform
  [Tad](https://www.tadviewer.com) to view parquet files on desktop


## Directory structure

+ `models/`
  + Model-ready prepared datasets (under `dfx/`) not always populated
  + Model-specific:
    + posterior traces as `*.netcdf`
    + plate-notation diagrams as `graph*`
  + Usually accompanies code in `src/`

+ `prepared/`
  + These are pre-prepared files for use in EDA
  + We typically have to further prepare these for downstream modelling

+ `raw/verbatim/`
  + These are raw data files verbatim exactly as supplied from the client
  + We run hashes against these using `hash_verbatim_files.sh` to assign an
    SHA1 hash with datetime to help with data lineage and governance
  + These files are typically received in `.xls*` or `.*db` or `.*sv` formats
      and are usually very dirty

+ `raw/extracted/`
  + These are raw data files extracted from the initial dump to make it
    easier to work with the data e.g. to pull a table from an Excel file
  + These files are typically in `.csv` format and content still dirty
  + NOTE: In the rare case that we have direct DB access, we draw static
    datasets from the DB using SQL scripts in `sql/`, controlled by Python
    scripts in `src/` and store the results here in `raw/extracted/` in `.csv`
    or `.parquet` format. We assume that we can always recover the static
    data from the DB, hence we call it "extracted" and not "verbatim".

+ `raw/cleaned/`
  + These are raw files following an initial clean to address simple issues
    with datatypes and elements (missing data, invalid values)
  + These files are typically in `.parquet` format for optimised storage

+ `synthetic/`
  + These files are synthetically generated by code in the repo
  + These files are typically in `.parquet` format for optimised storage.


## Terminology

IMPORTANT NOTE on terminology / naming convention and dataset partitioning
based on the information present and dataset usage.

```text
Dataset terminology / partitioning / purpose:

|<---------- Relevant domain of all data for our analyses & models ---------->|
|<----- "Observed" historical target ------>||<- "Unobserved" future target ->|
|<----------- "Working" dataset ----------->||<----- "Forecast" dataset ----->|
|<- Training/CrossVal ->||<- Test/Holdout ->|
```

+ The **"Observed"** historical target dataset has:
  + a _known_ exogenous (target) feature value
  + known endogenous feature values to allow model regression
  + a hypothetical structure that we use to design the model

  + The **"Working"** dataset is the same as this "Observed" data, and may be
    split into:
    + A **Training/CrossVal** set used to fit the model. This may be
      partitioned into multiple Cross-Validation sets if required by the model
      architecture and fitting process
    + A **Test/Holdout** used to evaluate the model fit against a known target
    + We can use this Working set _in full_ when fitting the final model for
      Production, because this yields the most performant model

+ The **"Unobserved"** future target dataset has:
  + an _unknown_ exogenous (target) feature value
  + known endogenous feature values to allow model regression
  + a hypothetical structure that we use to design the model

  + The **"Forecast"** dataset is the same as this "Unobserved" data, and is
    generally what we will try to predict upon in Production
    + We might create predictions for individual datapoints or in bulk
    + _If_ the entities in the data evolve over time (e.g. a set of policies
      each with evolving premium payments and claim developments),
      and _if_ the endogenous features don't evolve with time (they are static
      not dynamic) then we can artificially create a Forecast dataset by
      extending the Working dataset forward in time.

Further note:

+ We may refer to "In-Sample" and "Out-of-Sample" datasets. The
  former is the data used to train the model and the latter to evaluate the
  model against a _known_ exogenous (target) value or forecast an _unknown_
  exogenous (target) value. So they can be used during Working or Forecasting.

+ Strictly speaking, our Bayesian modelling workflow does not require us to
  evaluate the model on a Test/Holdout set because we can use in-sample
  Pareto-smoothed Leave-One-Out (LOO-PIT) cross-validation testing. This is more
  powerful, and lets us fit & evaluate the model using the full Working set.

+ However, purely to aid reader comprehension and demonstrate the out-of-sample
  prediction workflow, we may use the practice of a known Test/Holdout set.

---
Oreum OÃœ &copy; 2024
